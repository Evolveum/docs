= Smart Correlation in midPoint 4.6 and Beyond
:toc:

== Introduction

The "smart correlation" is a mechanism to correlate identity data to existing focus objects in the
repository. Typical use is e.g. the _resource objects correlation_ during the synchronization
process, where (previously unknown) accounts on a resource are synchronized to midPoint.
Another typical use will be the correlation during manual or automated registration of new users,
including self-registration.

In midPoint 4.4 and before, the only way of correlation was the use of correlation filters,
with strict binary output: either a matching object was found, or there was no match.

In midPoint 4.5, we introduced manual correlation for situations where there is a candidate match
(or more candidate matches) that need to be resolved by the human operator. Moreover, multiple
correlation mechanisms have been created: a custom script, call to an external ID Match API service,
or simplified, item-based correlation.

The goal for midPoint 4.6 and beyond is to provide a configurable correlation mechanism that
can work with approximate matching. For short, it is called _smart correlation_.

== High-Level Requirements

. The correlation mechanism must support _approximate matching_. In other words, it is not
sufficient to match a given data item (e.g., surname) only based on pure equality, probably
after some normalization. The solution must support formulating correlation rules using
metrics like, e.g., Levenshtein distance, Jaro distance, Jaro-Winkler distance, or phonetics
based matching (Soundex, Metaphone, etc.).

. The correlation mechanism must support multiple _variants_ of identity data.
For example, a woman may have a different name before and after marriage. Or, data about a person
coming from the student information system may be a little different from data about the same
person coming from the human resources system. We may want to keep both variants of the data
when considering that person during the correlation of newly-arrived identity data.
The treatment of identity data (e.g., whether to keep "older" variants or not) should be
configurable and probably manageable by a human operator on a case-by-case basis.

. The correlation mechanism should be _adaptive_. It should take human decisions into account
in its future executions.

=== Limitations for 4.6

- Requirement #3 (adaptivity) will not be implemented.
- Requirement #2 (multiple variants) will be implemented in limited way: Only a single variant
per data source (e.g. focus object or a given resource) will be maintained. Moreover, the variant
data will be maintained automatically, that is, manual corrections will not be supported.

== Some Design Decisions

=== Correlation Rules

In 4.6, the correlation mechanism will be based on _rules_, technically called _correlators_.
A rule can state that "if the family name, date of birth, and the national-wide ID all match,
then the identity is the same". Another rule can state that "if (only) the national-wide ID matches,
then the identity is the same with the confidence level of 0.7" (whatever the number means).

Rules reference _correlation items_. A correlation item is a prism item (currently, it must be
a property) of the correlated focus object, e.g., a user.

==== An Example

Let us consider the following _correlation items_.

.Sample correlation items
[%header]
[%autowidth]
|===
| Item name | Description | Item path
| `givenName` | Given name | `givenName`
| `familyName` | Family name | `familyName`
| `dateOfBirth` | Date of birth | `extension/dateOfBirth`
| `nationalId` | National-wide identifier (like social security number) | `extension/nationalId`
|===

We can use them to formulate e.g. the following rules:

.Sample set of correlation rules
[%header]
[%autowidth]
|===
| Rule# | Situation | Resulting confidence
| 1
| `familyName`, `dateOfBirth`, and `nationalId` exactly match
| Sure identity match
| 2
| `nationalId` exactly matches
| 0.8
| 3
| `givenName`, `familyName`, `dateOfBirth` exactly match
| 0.7
| 4
| `dateOfBirth` and first 5 characters of `familyName` matches
| 0.5
| 5
| `familyName` matches with Levenshtein distance (`LD`) between 1 and 4
| 0.5 - `LD`/10
|===

Rule #4 is just an example of using a custom normalization of a correlation item,
namely, taking the first five characters of it.

Rule #5 is an example of dynamically-computed confidence - in this case, based
on the specific value of the Levenshtein distance between identity data in question
and the candidate identity whose match is being considered.

=== Implementation Options

There are two basic implementation options.

==== Option 1: Using Existing Data

All correlation-related queries are issued against existing data, typically in the `m_user` table.
No extra database tables need to be created.

The main disadvantage of this approach is that we are limited to a single variant of the data:
the current ones stored in the focus object (e.g., a user). The reason is that although it is
possible to use other variants of the data, there is currently no suitable place where the
variants could be stored. For example, their storage in assignments is more a hack than
a serious solution, because assignments are not meant to do this. Their storage in shadow objects,
as an alternative that has been considered as well, is limited to a specific use, namely
to resource objects correlation, and would not fit registration or self-registration scenarios.
This means the following:

. The configuration needed to access variants of data in custom places is too complex. Moreover,
the maintenance of data variants in custom places requires a lot of coding. Both these factors
can be seen in experimental examples in midPoint 4.5.

. Maintaining historic variants of the data, i.e., those that have been overwritten already
(either in the repository object or in resource objects), requires even more custom coding.

==== Option 2: Using Separate Correlation Data Container

Here we put all correlation-related data into a special _correlation data container_ that may look
like this:

.Correlation data container sample
[source, xml]
----
<user>
    <!-- ... other data ... -->
    <correlation>
        <variant id="1">
            <source xsi:type="FocusCorrelationDataSourceType"/>
            <items>
                <original>
                    <givenName>Alice</givenName>
                    <familyName>Green</familyName>
                    <dateOfBirth>1997-01-01</dateOfBirth>
                    <nationalId>9751013333</dateOfBirth>
                </original>
                <normalized>
                    <givenName>alice</givenName>
                    <familyName>green</familyName>
                    <familyName.5>green</familyName>
                    <dateOfBirth>1997-01-01</dateOfBirth>
                    <nationalId>9751013333</dateOfBirth>
                </normalized>
            </items>
        </variant>
        <variant id="2">
            <source xsi:type="ProjectionCorrelationDataSourceType">
                <shadowRef oid="43fb79a3-d22d-480d-aa85-e04aa4749d46"/>
            </source>
            <!-- to save space, can we put here multiple sources (if the data is the same)? -->
            <items>
                <original>
                    <givenName>Alice</givenName>
                    <familyName>Johnson</familyName>
                    <dateOfBirth>1997-01-01</dateOfBirth>
                    <nationalId>9751013333</dateOfBirth>
                </original>
                <normalized>
                    <givenName>alice</givenName>
                    <familyName>johnson</familyName>
                    <familyName.5>johns</familyName.5>
                    <dateOfBirth>1997-01-01</dateOfBirth>
                    <nationalId>9751013333</dateOfBirth>
                </normalized>
            </items>
        </variant>
        <!-- Another way of treating alternative normalizations? But what about the queries? -->
        <variant id="3">
            <source xsi:type="ProjectionCorrelationDataSourceType">
                <shadowRef oid="461f1aca-73d3-41e9-bbed-64b636f72520"/>
            </source>
            <items>
                <original>
                    <givenName>Alice</givenName>
                    <familyName>Johnson</familyName>
                    <dateOfBirth>1997-01-01</dateOfBirth>
                    <nationalId>9751013333</dateOfBirth>
                </original>
                <normalized id="1">
                    <givenName>alice</givenName>
                    <familyName>johnson</familyName>
                    <!-- avoiding values that are equal -->
                </normalized>
                <normalized id="2">
                    <identifier>first-5-characters</identifier>
                    <familyName>johns</familyName>
                    <!-- avoiding values that are equal -->
                </normalized>
            </items>
        </variant>
    </correlation>
</user>
----

We need to have both original version of the data and the normalized one. The former is needed,
for example, to show correlation options in the GUI. The latter is needed for the actual matching.

_Do we need both versions in the XML form?_

After all, we do not do the same for polystring data: usually, we store only the original form
in XML, and repository does the normalization itself. In this case, however, the normalization
algorithms are focus/archetype/object-type specific, so it makes sense they are executed
in `model-impl` module.

But, indeed, we might make this data _index-only_ - or, at least, factored out from the main XML.
We will decide on this in the course of the development.

_What about the alternative normalizations?_

The names like `familyName.5` seem to be clumsy. Are there other options? Maybe something like
this:

.Other way of storing alternative normalizations
[source, xml]
----
<user>
    <!-- ... other data ... -->
    <correlation>
        <variant id="1">
            <source> ... </source>
            <item>
                <identifier>givenName</identifier>
                <value>Alice</value>
                <normalized>
                    <value>alice</value>
                </normalized>
            </item>
            <item>
                <identifier>familyName</identifier>
                <value>Johnson</value>
                <normalized>
                    <value>johnson</value>
                </normalized>
                <normalized>
                    <identifier>first-5-characters</identifier>
                    <value>johns</value>
                </normalized>
            </item>
            <item>
                <identifier>dateOfBirth</identifier>
                <value>1997-01-01</value>
                <!-- normalized is the same as the original -->
            </item>
            <item>
                <identifier>nationalId</identifier>
                <value>9751013333</value>
                <!-- normalized is the same as the original -->
            </item>
        </variant>
        <!-- other variants -->
    </correlation>
</user>
----

The main issue with those alternative representations is how we should query them.
The original (plain) representation can be queried like this:

[source,axiom]
----
correlation/variant matches (
    items/normalized/givenName =[levenshtein(0,3)] 'alice'
    and items/normalized/familyName.5 =[levenshtein(0,1)] 'johns'
    and items/normalized/dateOfBirth = '1997-01-01'
)
----

The alternative representations would require either some new query features, like

[source,axiom]
----
correlation/variant matches (
    item[identifier='givenName']/normalized[identifier is null]/value='alice'
)
----

Or, there would need to be some magic during query interpretation, e.g., using
fake prism structures to query the data.

The maintenance of this container is semi-automatic: It is carried out by midPoint itself,
according to specified rules, e.g., whether to keep historic records related to changes like
surname being changed after a marriage, or to fixing typos in the data; or whether to keep data
specific to individual source resources (like student information system or human resources system).
Additionally, the data can be corrected, added, or deleted manually by an operator.

Here are two implementation options.

===== Option 2a: Custom Correlation Table

The data can be stored in a custom correlation database table like this:

.Sample correlation table
[%header]
[%autowidth]
|===
| OID | Variant ID | givenName | familyName | familyName5 | dateOfBirth | nationalId
| 081168ee-de54-4005-9bdd-a6c55d7fcef7
| 1
| alice
| green
| green
| 1997-01-01
| 9751013333

| 081168ee-de54-4005-9bdd-a6c55d7fcef7
| 2
| alice
| johnson
| johns
| 1997-01-01
| 9751013333

| 0d49b6ff-7143-4afa-a02f-0abd84f3201d
| 1
| jack
| sparrow
| sparr
| 1691-01-01
| 9101014444
|===

Such a table would be - at least initially - created _externally_, i.e., by the person deploying
midPoint. It would be mapped to midPoint data using the correlation configuration.

===== Option 2b: Embedded Correlation Data

An alternative solution (not requiring a custom correlation table) is to use a JSONB-typed table
column right in the appropriate table (like `m_user`) - in the same way as `extension` is stored
today. This approach may be a bit less efficient but dramatically easier to set up: no custom
table creation is required.

The main disadvantages of option 2 (both 2a and 2b) are the implementation and administration
complexity. We would need to implement a mechanism that would keep the source data (in user,
shadows, assignments, and the like) in sync with the correlation data container; including
some rules driving that. And the deployer would need to configure that mechanism.

=== The Suggested Way Forward for 4.6

Because we need to provide - at least - roughly the functionality if ID Match API, we need some
support for correlation data variants. Therefore, we have to go with option 2, presumably 2b.

We know we are able to issue fuzzy searches (e.g. using Levenshtein distance) also against
JSONB-encoded data stored in extension container. For example,

[source,sql]
----
SELECT *, levenshtein(ext->>'1','alex') FROM m_user WHERE levenshtein(ext->>'1','alex') < 3;
----

Therefore, the following is suggested:

. Enhance Query API so that it will support selected approximate search features. As a minimum,
Levenshtein edit distance will be supported. The exact form is to be decided, e.g., if the support
will be based on a new clause, a new matching rule, or a newly-added "equal" clause option.
That way or another, we need to specify Levenshtein distance bound or bounds, and - eventually -
an option to return the measured distance as part of the result set. (Otherwise, if we would like
to reflect the distance in the metric, we would need to compute it ourselves.)
- Requirements specification (i.e. what are the required options): *Tadek*, *Pavol*
- Implementation: *Rišo* or *Pavol*

. Implement the new Query API features in the native repository.
- By: *Rišo* and *?*

. Implement the new correlation data container in the native repository.
- By: *Rišo* and *?*

. Implement the functionality to update the correlation data container.
- By: *Pavol* with the help of *Tadek*

. Update the correlation configuration language (see xref:configuration.adoc[separate document]).
- By: *Pavol* with the help of *Tadek*

. Update the correlators to support uncertainty, confidence levels, and variants
- By: *Pavol* with the help of *Tadek*

. Update the GUI to show certainty levels (and other modifications as needed)
- By: *?*

. Prepare tests and documentation
- By: *Tadek* and *Pavol*
