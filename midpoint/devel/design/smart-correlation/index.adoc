= Smart Correlation in midPoint 4.6 and Beyond

== Introduction

The "smart correlation" is a mechanism to correlate identity data to existing focus objects in the
repository. Typical use is e.g. the _resource objects correlation_ during the synchronization
process, where (previously unknown) accounts on a resource are synchronized to midPoint.
Another typical use will be the correlation during manual or automated registration of new users,
including self-registration.

In midPoint 4.4 and before, the only way of correlation was the use of correlation filters,
with strict binary output: either a matching object was found, or there was no match.

In midPoint 4.5, we introduced manual correlation for situations where there is a candidate match
(or more candidate matches) that need to be resolved by the human operator. Moreover, multiple
correlation mechanisms have been created: a custom script, call to an external ID Match API service,
or simplified, item-based correlation.

The goal for midPoint 4.6 and beyond is to provide a configurable correlation mechanism that
can work with approximate matching. For short, it is called _smart correlation_.

== High-Level Requirements

. The correlation mechanism must support _approximate matching_. In other words, it is not
sufficient to match a given data item (e.g., surname) only based on pure equality, probably
after some normalization. The solution must support formulating correlation rules using
metrics like, e.g., Levenshtein distance, Jaro distance, Jaro-Winkler distance, or phonetics
based matching (Soundex, Metaphone, etc.).

. The correlation mechanism must support multiple _variants_ of identity data.
For example, a woman may have a different name before and after marriage. Or, data about a person
coming from the student information system may be a little different from data about the same
person coming from the human resources system. We may want to keep both variants of the data
when considering that person during the correlation of newly-arrived identity data.
The treatment of identity data (e.g., whether to keep "older" variants or not) should be
configurable and probably manageable by a human operator on a case-by-case basis.

. The correlation mechanism should be _adaptive_. It should take human decisions into account
in its future executions.

=== Limitations for 4.6

- Requirement #3 (adaptivity) will not be implemented.
- Requirement #2 (multiple variants) will be implemented if allowed by the time available.

== Some Design Decisions

=== Correlation Rules

In 4.6, the correlation mechanism will be based on _rules_. A rule can state that
"if the family name, date of birth, and the national-wide ID all match, then the identity
is the same". Another rule can state that "if (only) the national-wide ID matches, then
the identity is the same with the confidence level of 0.7" (whatever the number means).

Rules reference _correlation items_. A correlation item is a prism item (currently, it must be
a property) of the correlated focus object, e.g., a user.

==== An Example

Let us consider the following _correlation items_.

.Sample correlation items
[%header]
[%autowidth]
|===
| Item name | Description | Item path
| `givenName` | Given name | `givenName`
| `familyName` | Family name | `familyName`
| `dateOfBirth` | Date of birth | `extension/dateOfBirth`
| `nationalId` | National-wide identifier (like social security number) | `extension/nationalId`
|===

We can use them to formulate e.g. the following rules:

.Sample set of correlation rules
[%header]
[%autowidth]
|===
| Rule# | Situation | Resulting confidence
| 1
| `familyName`, `dateOfBirth`, and `nationalId` exactly match
| Sure identity match
| 2
| `nationalId` exactly matches
| 0.8
| 3
| `givenName`, `familyName`, `dateOfBirth` exactly match
| 0.7
| 4
| `dateOfBirth` and first 5 characters of `familyName` matches
| 0.5
| 5
| `familyName` matches with Levenshtein distance (`LD`) between 1 and 4
| 0.5 - `LD`/10
|===

Rule #4 is just an example of using a custom normalization of a correlation item,
namely, taking the first five characters of it. (There may be a default normalization as well,
like taking lower-cased string data, with any diacritic marks stripped off.)

Rule #5 is an example of dynamically-computed confidence - in this case, based
on the specific value of the Levenshtein distance between identity data in question
and the candidate identity whose match is being considered.

=== Implementation Options

There are two basic implementation options.

==== Option 1: Using Existing Data

All correlation-related queries are issued against existing data, typically in the `m_user` table.
No extra database tables need to be created.

The main disadvantage of this approach is that we are limited to a single variant of the data:
the current ones stored in the focus object (e.g., a user). The reason is that although it is
possible to use other variants of the data, there is currently no suitable place where the
variants could be stored. For example, their storage in assignments is more a hack than
a serious solution, because assignments are not meant to do this. Their storage in shadow objects,
as an alternative that has been considered as well, is limited to a specific use, namely
to resource objects correlation, and would not fit registration or self-registration scenarios.
This means the following:

. The configuration needed to access variants of data in custom places is too complex. Moreover,
the maintenance of data variants in custom places requires a lot of coding. Both these factors
can be seen in experimental examples in midPoint 4.5.

. Maintaining historic variants of the data, i.e., those that have been overwritten already
(either in the repository object or in resource objects), requires even more custom coding.

==== Option 2: Using Separate Correlation Data Container

Here we put all correlation-related data into a special _correlation data container_ that may look
like this:

[source, xml]
----
<user>
    <!-- ... other data ... -->
    <correlation>
        <variant id="1">
            <item>
                <identifier>givenName</identifier>
                <value>Alice</value>
            </item>
            <item>
                <identifier>familyName</identifier>
                <value>Green</value>
            </item>
            <item>
                <identifier>dateOfBirth</identifier>
                <value>1997-01-01</value>
            </item>
            <item>
                <identifier>nationalId</identifier>
                <value>9751013333</value>
            </item>
        </variant>
        <variant id="2">
            <item>
                <identifier>givenName</identifier>
                <value>Alice</value>
            </item>
            <item>
                <identifier>familyName</identifier>
                <value>Johnson</value>
            </item>
            <item>
                <identifier>dateOfBirth</identifier>
                <value>1997-01-01</value>
            </item>
            <item>
                <identifier>nationalId</identifier>
                <value>9751013333</value>
            </item>
        </variant>
    </correlation>
</user>
----

The maintenance of this container is semi-automatic: It is carried out by midPoint itself,
according to specified rules, e.g., whether to keep historic records related to changes like
surname being changed after a marriage, or to fixing typos in the data; or whether to keep data
specific to individual source resources (like student information system or human resources system).
Additionally, the data can be corrected, added, or deleted manually by an operator.

The search is then carried out on this data.

Here are two implementation options.

===== Option 2a: Custom Correlation Table

The data can be stored in a custom correlation database table like this:

.Sample correlation table
[%header]
[%autowidth]
|===
| OID | Variant ID | givenName | familyName | familyName5 | dateOfBirth | nationalId
| 081168ee-de54-4005-9bdd-a6c55d7fcef7
| 1
| alice
| green
| green
| 1997-01-01
| 9751013333

| 081168ee-de54-4005-9bdd-a6c55d7fcef7
| 2
| alice
| johnson
| johns
| 1997-01-01
| 9751013333

| 0d49b6ff-7143-4afa-a02f-0abd84f3201d
| 1
| jack
| sparrow
| sparr
| 1691-01-01
| 9101014444
|===

Such a table would be - at least initially - created _externally_, i.e., by the person deploying
midPoint. It would be mapped to midPoint data using the correlation configuration.

===== Option 2b: Embedded Correlation Data

An alternative solution (not requiring a custom correlation table) is to use a JSONB-typed table
column right in the appropriate table (like `m_user`) - in the same way as `extension` is stored
today. This approach may be a bit less efficient but dramatically easier to set up: no custom
table creation is required.

The main disadvantages of option 2 (both 2a and 2b) are the implementation and administration
complexity. We would need to implement a mechanism that would keep the source data (in user,
shadows, assignments, and the like) in sync with the correlation data container; including
some rules driving that. And the deployer would need to configure that mechanism.

=== The Suggested Way Forward for 4.6

We will probably go with the first option, i.e., using existing data to execute correlation queries.

We know we are able to issue fuzzy searches (e.g. using Levenshtein distance) also against
JSONB-encoded data stored in extension container. For example,

[source,sql]
----
SELECT *, levenshtein(ext->>'1','alex') FROM m_user WHERE levenshtein(ext->>'1','alex') < 3;
----

Therefore, the following is suggested:

. Enhance Query API so that it will support selected approximate search features. As a minimum,
Levenshtein edit distance will be supported. The exact form is to be decided, e.g., if the support
will be based on a new clause, a new matching rule, or a newly-added "equal" clause option.
That way or another, we need to specify Levenshtein distance bound or bounds, and - eventually -
an option to return the measured distance as part of the result set. (Otherwise, if we would like
to reflect the distance in the metric, we would need to compute it ourselves.)
- Requirements specification (i.e. what are the required options): *Tadek*, *Pavol*
- Implementation: *RiÅ¡o* or *Pavol*

. Implement the new Query API features in the native repository.
- By: *?*

. Update the correlation configuration language (see a separate document).
- By: *Pavol* with the help of *Tadek*

. Update the correlators to support uncertainty and confidence levels
- By: *Pavol* with the help of *Tadek*

. Update the GUI to show certainty levels (and other modifications as needed)
- By: *?*

. Prepare tests and documentation
- By: *Tadek* and *Pavol*
