= Perfomance Tests Ideas
:page-wiki-name: Perfomance Tests Ideas
:page-wiki-id: 24674306
:page-wiki-metadata-create-user: semancik
:page-wiki-metadata-create-date: 2017-05-24T10:25:54.418+02:00
:page-wiki-metadata-modify-user: semancik
:page-wiki-metadata-modify-date: 2017-05-24T10:32:51.215+02:00
:page-upkeep-status: yellow

What to test:

* Import with approx.
1M identities (CSV import source, just create user in midPoint).
To check: is import stable? Is there a memleak? Does it slow down? How fast? Does the slowdown stop eventually? What is the sustained import speed?

* Midification throughput: how many user modifications can we do per second? (No resource modifications yet).
E.g. use REST interface.
How does it scale? Is the speed different if we have 10K identities or 500K identities? How does it scale with multiple client threads? E.g. Does using 4 client threads increase the throughput? How about 16 client threads?

* How does new resources influence the import/modification.
If we add new LDAP resource, what is the impact on import/modification speed? How about adding 3 LDAP resoruces?

* What is the impact of many assignments (many roles, many orgs) for a single user.
How is the recompute speed affected? How is the modification throughput affected? We are looking for approx.
10-100 roles per user.

** To explore: midPoint now reads every role one by one using getObject operation.
Each one is a separate database roundtrip.
Whould it be more efficient to get all the user roles (assignment targets) in one big operation using a database join? How much more efficient?

* Extended properties: how does number of extended properties influence the performance.
E.g. if there is 30 custom user properties, all populated on import, does it influence import speed? How much? What is the comparative impact of using 10 native properties (locality, employeeNumber, etc.) versus the impact of using custom 10 extension properties.