= Cluster deployment
:page-nav-title: Midpoint cluster
:toc:
:toclevels: 4

== What is it about

This page will cover use cases when more midpoint nodes in the cluster configuration is use.
This page is extending the information valid for xref:/midpoint/install/kubernetes/single-node.adoc[single node] deployment.

In case you need working sample you can go directly to the part related to <<_nfs>>  (it is recommended deployment option to fulfill several requirements).

== Cluster of midpoint nodes

There are few things which has to be handled to be able to operate midpoint in cluster - more cooperating nodes.

* taskManager
* nodeID
* keystore

Once all the "cluster requirements" is met you can increase amount of replicas in statefulset definition for midpoint.

[NOTE]
In case of statefulset the suffix of the pod is increasing order.
First created pod has suffix *-0*.
In case you increase the amount of replicas the pod are added "to the end" of the list *-1*, *-2*, *-3*, etc.
In case you are decreasing the amount of replicas the latest one is removed.
It is not possible to remove the pod "in the middle" of the list.
This may be important in case of utilizing persistent volumes for the pods.

Midpoint pods can be operated even without persistent volumes as the important objects are stored in the repository and shared between the nodes.
The areas which may need specific handling:

* logs +
to not lost the records after removing / re-creating the pod

* connectors +
It can be distributed using shared object (configMap, R/O shared volume between the pods, etc.)

* exports / reports +
In some situation the output can be stored in the filesystem.
In that case we probably prefer to keep the files even after re-creating the pod.

The list is example and it is not have to be complete.
The design of the deployment may contain other specific objects to handle.

=== Task Manager

Midpoint's task manager has to be run clustered.
This setting has to be added to all nodes.

[source,kubernetes]
...
          env:
            - name: MP_SET_midpoint_taskManager_clustered
              value: true
...

=== nodeID

Node as cluster member has to have the unique ID.
To be able to run the node there have to be set the way how to generate / set the ID.
As the pods name are unique (generated) we can use hostname for this purpose.
To do that we need to set one additional environment variable.

[source,kubernetes]
...
          env:
            - name: MP_SET_midpoint_nodeIdSource
              value: hostname
...

=== keystore

Keystore is generated with the start in case it is not available.
The result is that each node would generate "own" key in the keystore and the object will be readable only by the node which has created it.
To address this "issue" we have to prepare the keystore to be available to each node once it is starting.

* share the file

** network share +
Kubernetes natively offer mount NFS store as volume to pod.
We can share the space and first node will generate it.
All other node or even this node after restart / recreate will use the file so the key for description will be available.
Using persistent volume for NFS server is good idea.
+
The issue may happen once two pods would start in parallel and both would want to generate it.

** volume share +
Not all the drivers offer concurrent write access to the volume.
This option not necessary have to be available in general.

* pre-generate the file into secret object and share it with all the pods as mounted volume +
For the testing purpose this approach offer sharing the keystore even between the whole env deployment.

==== NFS mount

<<NFS>> has dedicated part of this document.

To mount volume you can use syntax like this example:

.example for NFS volume
[source]
...
    spec:
      volumes:
        - name: nfs-poi
          nfs:
            server: mp-demo-nfs.mp-demo.svc.cluster.local
            path: /exports/poi
...

[NOTE]
FQDN resolution is done by kubelet at the moment of starting the pod.
In case the node is not able to resolve the FQDN (e.g. it is using "external" DNS server) the mounting may fail.
If you face this issue please change the DNS server in the node's */etc/resolv.conf* or use cluster IP of the service object instead of FQDN.

Once the volume for pod is ready it can be used in container definition in volumeMounts section.

==== keystore generating

[TIP]
====
This not have to be handled in case you will use NFS for the midpoint home.
The keystore would be generated by the first midpoint node and all other nodes will have it available.

You can follow this even with NFS in case you prefer e.g. other than default key size.
====

In case you prefer to manually generate keystore the keytool could be used.
Midpoint is expecting jceks format of the keystore.

.generate new keystore (AES_128)
[source,bash]
keytool -genseckey -alias default -keystore keystore.jceks -storetype jceks -keyalg AES -keysize 128

After running of this file you are asked to provide password for the keystore.
This password is the password which is provided to midpoint by using keyStorePassword or keyStorePassword_FILE parameter.
The default is *changeit*.

You can use _storepass_ to set the password for storage as parameter.

.example of generating with the storepass option
[source,bash]
keytool -genseckey -alias default -keystore keystore.jceks -storetype jceks -keyalg AES -keysize 128 -storepass changeit

[#_nfs]
== NFS

NFS volume is natively supported with the kubernetes (it is described e.g. in kuberenetes documentation related to the link:https://kubernetes.io/docs/concepts/storage/volumes/#nfs[volumes]).

To have it working there are few thing which should be checked on kubernetes node:

* *NFS tools* available on the operating system +
Kubernetes call system tool to mount the NFS volume.
The required package name may differ based on the used distribution - on the debian based distribution (including ubuntu) the name of the package is *nfs-common*.

* DNS resolving +
In case we want to use "internal" cluster FQDN it has to be resolvable for the kubernetes' node OS.
by default the names are resolvable in the cluster but node's resolver may use "external" DNS server where the cluster FQDNs are not known.
The solution is point OS's resolver to the cluster "internal" IP as the node can communicate with any cluster "internal" IPs.

.example of the change on debian based distribution (e.g. IP of DNS is 10.96.0.10)
[source,bash]
cat << EOF >>/etc/systemd/resolved.conf
#[Resolve]
DNS=10.96.0.10
Domains=~cluster.local
EOF
systemctl restart systemd-resolved

=== server

.statefulset definition for server
[source,kubernetes]
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mp-demo-nfs
  namespace: mp-demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mp-demo-nfs
  template:
    metadata:
      labels:
        app: mp-demo-nfs
    spec:
      initContainers:
        - name: init-structure
          image: 'k8s.gcr.io/volume-nfs'
          command: ["/bin/bash","-c"]
          args:
            - mkdir -p /exports/poi ;
              echo "post-initial-objects folder has been created..."
          volumeMounts:
          - mountPath: /exports
            name: mp-demo-nfs-store
          imagePullPolicy: IfNotPresent
      containers:
        - name: mp-demo-nfs
          image: 'k8s.gcr.io/volume-nfs'
          command: ["/bin/bash", "/usr/local/bin/run_nfs.sh", "/exports"]
          ports:
            - name: nfs
              containerPort: 2048
              protocol: TCP
            - name: mountd
              containerPort: 20048
              protocol: TCP
            - name: rpvbind
              containerPort: 111
              protocol: TCP
          securityContext:
            privileged: true
          volumeMounts:
          - mountPath: /exports
            name: mp-demo-nfs-store
          imagePullPolicy: IfNotPresent
      restartPolicy: Always
      terminationGracePeriodSeconds: 10
  serviceName: mp-demo-nfs
  volumeClaimTemplates:
    - kind: PersistentVolumeClaim
      apiVersion: v1
      metadata:
        name: mp-demo-nfs-store
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 256Mi
        storageClassName: csi-rbd-hdd
        volumeMode: Filesystem

[NOTE]
====
There has been used the same image as in kubernetes documentation.
Feel free to use any other image containing nfs server tool you are familiar with.

There is used initContainer to create subfolder for poi (Post-Initial-Objects) for the next example with mounting just POI to midpoint home.
The mount would fail in case the directory is not available.
In case you prefer to have all midpoint home directory on NFS the initContainer is not needed.
====

.service definition for the server
[source,kubernetes]
apiVersion: v1
kind: Service
metadata:
  name: mp-demo-nfs
  namespace: mp-demo
spec:
  ports:
    - name: nfs
      port: 2049
    - name: mountd
      port: 20048
    - name: rpcbind
      port: 111
  selector:
    app: mp-demo-nfs

=== midpoint with NFS volume

.statefulset definition
[source,kubernetes]
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mp-pg-demo
  namespace: mp-demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mp-pg-demo
  template:
    metadata:
      labels:
        app: mp-pg-demo
    spec:
      volumes:
        - name: mp-home
          emptyDir: {}
        - name: db-pass
          secret:
            secretName: mp-demo
            defaultMode: 420
        - name: mp-poi
          configMap:
            name: mp-demo-poi
            defaultMode: 420
        - name: nfs-poi
          nfs:
            server: mp-demo-nfs.mp-demo.svc.cluster.local
            path: /exports/poi
      initContainers:
        - name: mp-config-init
          image: 'evolveum/midpoint:4.4-alpine'
          command: ["/bin/bash","/opt/midpoint/bin/midpoint.sh","init-native"]
          env:
            - name: MP_INIT_CFG
              value: /opt/mp-home
          volumeMounts:
            - name: mp-home
              mountPath: /opt/mp-home
          imagePullPolicy: IfNotPresent
      containers:
        - name: mp-pg-demo
          image: 'evolveum/midpoint:4.4-alpine'
          ports:
            - name: gui
              containerPort: 8080
              protocol: TCP
          env:
            - name: MP_ENTRY_POINT
              value: /opt/midpoint-dirs-docker-entrypoint
            - name: MP_SET_midpoint_repository_database
              value: postgresql
            - name: MP_SET_midpoint_repository_jdbcUsername
              value: midpoint
            - name: MP_SET_midpoint_repository_jdbcPassword_FILE
              value: /opt/midpoint/config-secrets/password
            - name: MP_SET_midpoint_repository_jdbcUrl
              value: jdbc:postgresql://mp-demo-db.mp-demo.svc.cluster.local:5432/midpoint
            - name: MP_UNSET_midpoint_repository_hibernateHbm2ddl
              value: "1"
            - name: MP_NO_ENV_COMPAT
              value: "1"
          volumeMounts:
            - name: mp-home
              mountPath: /opt/midpoint/var
            - name: db-pass
              mountPath: /opt/midpoint/config-secrets
            - name: mp-poi
              mountPath: /opt/midpoint-dirs-docker-entrypoint/post-initial-objects
            - name: nfs-poi
              mountPath: /opt/midpoint/var/post-initial-objects
          imagePullPolicy: IfNotPresent
  serviceName: mp-pg-demo

[WARNING]
The pod will not start (it will wait in state *PodInitializing*) until the NFS will be available. It can be unavailable as NFS server is not up yet or the FQDN can't be resolved. The reason can be find out in the pod's information.