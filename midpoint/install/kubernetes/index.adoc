= Kubernetes environment
:page-nav-title: Kubernetes
:toc:
:toclevels: 4

[NOTE]
The yaml samples are available also on the link:https://github.com/Evolveum/midpoint-kubernetes/tree/master/base_env[github].
There are available also commands how to use the files with *kubectl* in readme.adoc file.

== What is it about

This page (or subtree of the documentation) will describe environment related to the clouds.
As there may be differencies in configuration per specific environment, we will focus here on kubernetes.
The information is applicable for cloud environments in general but small configuration changes based on specific cloud environemnt might be required.

=== Images

As we are working in cloud environment the image is the most important part of the solution ( next to the configuration ).
All used images are available on public link:https://hub.docker.com/[Docker hub]:

* link:https://hub.docker.com/r/evolveum/midpoint[Midpoint] +
When we talk about the midpoint located in the cloud, the image of midpoint is a mandatory requirement.
* link:https://hub.docker.com/_/postgres[Postgres] +
The documentation will be mainly focused on native repository, which has postgres DB as a requirement so we can mention this image here.

The information related to the docker image mentioned in other parts of the documentation is also valid for our purposes.

== Type of instance

There can be identify several "type" of the instances / deployments.
Depends on the complexity (e.g. single node vs. clustered nodes) or configuration (e.g. native vs. Generic repository).

[NOTE]
To make the objects lgically grouped we are using <<Namespace>> mp-demo.
Namespace object type is mentioned <<Namespace,later>> in the document.

=== Single node - by the repository

Since Midpoint 4.4 there are two definition for the repository (where the data for the midpoint application are stored).

- xref:/midpoint/reference/repository/generic/[Generic] (deprecated since 4.4)
- xref:/midpoint/reference/repository/native-postgresql/[Native]

==== Generic

[NOTE]
Even the generic repository is deprecated it is so far still available and in some "lab" usecases may be useful.

The performance is lower in comparison with Native repository but the structure is created automatically once the connection to the databse is ready or H2 DB file is created.

This option is mentioned here to have complete information at one place but we are not recommend this for the production environment or (serious) test environment.

===== Quick start - H2

This example will start midpoint with default environment variables.
This is "kubernetes alternative" to run docker image directly with xref:/midpoint/install/docker/[docker].

.Example to "just run" with defaults
[source,kubernetes]
apiVersion: v1
kind: Pod
metadata:
  name: mp-h2-demo
  namespace: mp-demo
  labels:
    app: mp-h2-demo
spec:
  containers:
    - name: mp-h2-demo
      image: 'evolveum/midpoint:4.4-alpine'
      ports:
        - name: gui
          containerPort: 8080
          protocol: TCP
      imagePullPolicy: IfNotPresent
  restartPolicy: Always

The result will be midpoint running on Generic repository located in H2 DB file.
We are not handling config.xml file so the default one will be used.
There will be no shared secrects as it is not needed (e.g. needed in case of connection to the database).
The keystore will be generated with the start of the system so in principle (without extra steps) it is not possible to run it with more nodes (clustered).
From Midpoint point of view I would label this option as "hello world" - it is just up and running.

Before we will add next "layer" to the configuration I would re-declare it as statefulset (object which "control" pods).

.StatefulSet form  for "just run" with defaults
[source,kubernetes]
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mp-h2-demo
  namespace: mp-demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mp-h2-demo
  template:
    metadata:
      labels:
        app: mp-h2-demo
    spec:
      containers:
        - name: mp-h2-demo
          image: 'evolveum/midpoint:4.4-alpine'
          ports:
            - name: gui
              containerPort: 8080
              protocol: TCP
          imagePullPolicy: IfNotPresent
  serviceName: mp-h2-demo

Even we can see "replicas: 1" it is practically fixed value.
Don't try to increase this number (start cluster) as in this configuration it will not work.
Cluster configuration will be shown later in the document.

It could be also noted that in this configuration the service is available only internally in the Kubernetes environment.
Teoretically it can be routed but in principle IPs will be dynamic and it will change with every pod re-create.
In case you would like to make it "reachable" (the same IP, the same FQDN) please think about using <<Services>> and <<Ingress>> (of course with proper names and selectors).

===== Quick start - postgresql

Now we will add complexity in form of postgresql "store" for the midpoint objects.
So far we are still using deprecated generic repository to understand it and get it up and running in step-by-step approach.
The benefit now is auto-create of the database structure - we don't need to handle explicit db init.

We will need postgresql instance.
It make sense to use persistent volume for data store.

[CAUTION]
Without defining persistent store the data is located on the "shared" volume which is used by node itself.
In case of bigger amount of data and not so big storage for the node itself (e.g. local kubernetes cluster) the free space exhausting may occur.

At this moment we will define the postgresql without persistent volume to keep configuration in necessary minimum for easier understanding.

It has been already mentioned that IP adresses are dynamic.
Now we will create two pods - one for the midpoint and one for the postgresql.
To be able to predict interconnection we will define service for the database.
The connection will be realized via FQDN for the service.
The proper "linking" will be handle by the *selector* pointing to propper label.

.postresql db definition
[source,kubernetes]
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mp-demo-db
  namespace: mp-demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mp-demo-db
  template:
    metadata:
      labels:
        app: mp-demo-db
    spec:
      containers:
        - name: mp-demo-db
          image: 'postgres:13-alpine'
          ports:
            - name: db
              containerPort: 5432
              protocol: TCP
          env:
            - name: POSTGRES_INITDB_ARGS
              value: '--lc-collate=en_US.utf8 --lc-ctype=en_US.utf8'
            - name: POSTGRES_USER
              value: midpoint
            - name: POSTGRES_PASSWORD
              value: SuperSecretPassword007
          imagePullPolicy: IfNotPresent
      restartPolicy: Always
      terminationGracePeriodSeconds: 10
  serviceName: mp-demo-db

[NOTE]
You can see necessary init value for the authentication: +
*PGUSER:* midpoint +
*PGPASSWD:* SuperSecretPassword007 +
Feel free to change it but keep it consistent with following midpoint definition.
Otherwise the connection will not be established.

.service definition for the db ("meeting point" for the communication between midpoint and db)
[source,kubernetes]
apiVersion: v1
kind: Service
metadata:
  name: mp-demo-db
  namespace: mp-demo
spec:
  ports:
    - protocol: TCP
      port: 5432
      targetPort: 5432
  selector:
    app: mp-demo-db
  type: ClusterIP
  sessionAffinity: None

[NOTE]
The name is important for the FQDN construction.
In this documentation we will use default kubernetes domain - FQDN will be *<service_name>.<namespace>.svc.cluster.local*.
This domain may differ based on the environment setting.

.modpoint with repository located in the postgresql
[source,kubernetes]
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mp-pg-demo
  namespace: mp-demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mp-pg-demo
  template:
    metadata:
      labels:
        app: mp-pg-demo
    spec:
      containers:
        - name: mp-pg-demo
          image: 'evolveum/midpoint:4.4-alpine'
          ports:
            - name: gui
              containerPort: 8080
              protocol: TCP
          env:
            - name: MP_SET_midpoint_repository_database
              value: postgresql
            - name: MP_SET_midpoint_repository_jdbcUsername
              value: midpoint
            - name: MP_SET_midpoint_repository_jdbcPassword
              value: SuperSecretPassword007
            - name: MP_SET_midpoint_repository_jdbcUrl
              value: jdbc:postgresql://mp-demo-db.mp-demo.svc.cluster.local:5432/midpoint
          imagePullPolicy: IfNotPresent
  serviceName: mp-pg-demo

As far as we are using generic repository we can use "default" config.xml file.
All the changes can be overwritten during the start.
This is realized by the *MP_SET_** values which are handled by the xref:midpoint/install/midpoint-sh/[start script].
The name and content for the variables are related to the xref:midpoint/reference/repository/configuration/[repository configuration].

==== Native

xref:/midpoint/reference/repository/native-postgresql/[Native] repository came with midpoint 4.4.
For the purpose of deployment there is few specifics:

* DB related:
** it can be operated only on postgresql (postgresql's features has been utilizied during the design)
** the structure of the DB has to be initiated explicitly - midpoint expects already existing structure
* midpoint related:
** config.xml file has to be used

Postgresql should not be an issue as we will used official postgresql image.
The tricky part will be related to the second point - init the db structure.
Good for us is that all we need is already packed in the midpoint image.

===== The DB init

We will need just use what we have.
To realize it we will need to utilize init container in the kubernetes.
It is container (may be even more but in paralel not in sequence) which is run before main container.
The image for init container and container may differ.
To reach the requirement we will need "shared" volume between the conatiners in the pod.
It can be persistent volume but for now we will use emptyDir volume.
For serious deployment (even for the test) the persistent volime is good idea.
We will use midpoint image as init container and postgres image for "regular" container.

[source,kubernetes]
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mp-demo-db
  namespace: mp-demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mp-demo-db
  template:
    metadata:
      labels:
        app: mp-demo-db
    spec:
      volumes:
	- name: init-db
	  emptyDir: {}
      initContainers:
        - name: mp-db-init
          image: 'evolveum/midpoint:4.4-alpine'
          command: ["/bin/bash","/opt/midpoint/bin/midpoint.sh","init-native"]
          env:
            - name: MP_INIT_DB_CONCAT
              value: /opt/db-init/010-init.sql
          volumeMounts:
            - name: init-db
              mountPath: /opt/db-init
          imagePullPolicy: IfNotPresent
      containers:
        - name: mp-demo-db
          image: 'postgres:13-alpine'
          ports:
            - name: db
              containerPort: 5432
              protocol: TCP
          env:
            - name: POSTGRES_INITDB_ARGS
              value: '--lc-collate=en_US.utf8 --lc-ctype=en_US.utf8'
            - name: POSTGRES_USER
              value: midpoint
            - name: POSTGRES_PASSWORD
              value: SuperSecretPassword007
          volumeMounts:
            - name: init-db
              mountPath: /docker-entrypoint-initdb.d/
          imagePullPolicy: IfNotPresent
      restartPolicy: Always
      terminationGracePeriodSeconds: 10
  serviceName: mp-demo-db-service

[NOTE]
The preparation of init-db volume will happen with all the restarts of the DB's pod.
The init process of the DB will be done only once - only in case the db data is not found.
The image version (tag) have to be the same for init container of the DB and for the midpoint itself.
This way the initialized structure will correspond with the version of midpoint you are deploying.

It is possible to utilize *Secret* objects to store the password instead of keeping it in the configuration of the *StatefulSets* directly.

Once utilizing *Secrets* you can choice between more approaches.

* _mount the value as a file to pod's filesystem_ +
The mounting of the value as a file is the same like in case of config map.
The example is shown further in the document.
* pointing to the value

To point the value you can replace the definition:

.password stored directly in the object definition
[source,kubernetes]
...
          env:
            - name: POSTGRES_PASSWORD
              value: SuperSecretPassword007
...

with the following definition:

.password linked to the secret object from the object definition
[source,kubernetes]
...
          env:
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mp-demo
                  key: password
...

[NOTE]
This example expect to have the secret object with the name *mp-demo* in the same namespace as object where it is used (*mp-demo*).
The value which will be used is content of the key *password* located in the secret object.

===== DB Service definition

We will need to have the service definition so we can target the DB in midpoint configuration.
Without the service we are not able to predict "meeting point" in case of IP.
In some specific cases we can predict FQDN of the pod but using Service for this purpose is more than good idea.
It is even recommended approach - search for kubernetes related resources for more information if needed.

.service definition for the db ("meeting point" for the communication between midpoint and db)
[source,kubernetes]
apiVersion: v1
kind: Service
metadata:
  name: mp-demo-db
  namespace: mp-demo
spec:
  ports:
    - protocol: TCP
      port: 5432
      targetPort: 5432
  selector:
    app: mp-demo-db
  type: ClusterIP
  sessionAffinity: None

===== Midpoint

To start midpoint with the native repository the "custom" config.xml file has to be used.
There is audit related configuration which differs from "default" config.xml and it can't be overwritten by the MP_SET_* variables.
The sample config.xml for native repository is also delivered with midpoint image.
With this sample config.xml we have all we need to be able to set all the rest values using MP_SET_* variables.

We will use init container the similar way like in case of DB init.
In this case both init container and container will use the same image.

For this documentation purpose we will use the "emptyDir" definition for the volume.
This volume will be used for /opt/midpoint/var folder.
Based on your deployment specifics you may think about proper volume type. 

.modpoint with native repository located in the postgresql
[source,kubernetes]
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mp-pg-demo
  namespace: mp-demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mp-pg-demo
  template:
    metadata:
      labels:
        app: mp-pg-demo
    spec:
      volumes:
        - name: mp-home
          emptyDir: {}
      initContainers:
        - name: mp-config-init
          image: 'evolveum/midpoint:4.4-alpine'
          command: ["/bin/bash","/opt/midpoint/bin/midpoint.sh","init-native"]
          env:
            - name: MP_INIT_CFG
              value: /opt/mp-home
          volumeMounts:
            - name: mp-home
              mountPath: /opt/mp-home
          imagePullPolicy: IfNotPresent
      containers:
        - name: mp-pg-demo
          image: 'evolveum/midpoint:4.4-alpine'
          ports:
            - name: gui
              containerPort: 8080
              protocol: TCP
          env:
            - name: MP_SET_midpoint_repository_database
              value: postgresql
            - name: MP_SET_midpoint_repository_jdbcUsername
              value: midpoint
            - name: MP_SET_midpoint_repository_jdbcPassword
              value: SuperSecretPassword007
            - name: MP_SET_midpoint_repository_jdbcUrl
              value: jdbc:postgresql://mp-demo-db.mp-demo.svc.cluster.local:5432/midpoint
            - name: MP_UNSET_midpoint_repository_hibernateHbm2ddl
              value: "1"
            - name: MP_NO_ENV_COMPAT
              value: "1"
          volumeMounts:
            - name: mp-home
              mountPath: /opt/midpoint/var
          imagePullPolicy: IfNotPresent
  serviceName: mp-pg-demo


Once you pass the passwords (e.g. for keystore or database) as MP_SET_* parameter it is visible in "About" as text under "JVM properties".
The more secure option may be use password_FILE instead of password value.

The handling the secret and configMap objects are very similar.
To save some sample config iteration we can direclty show also the post-initial-import.
For this purpose xref:/midpoint/install/midpoint-sh/[Star script] offer entry point feature.
To use it the parameter *MP_ENTRY_POINT* can be set.
In the following example there are 2 XML files defined in mp-demo-poi configMap.

.modpoint with DB auth password in the file
[source,kubernetes]
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mp-pg-demo
  namespace: mp-demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mp-pg-demo
  template:
    metadata:
      labels:
        app: mp-pg-demo
    spec:
      volumes:
        - name: mp-home
          emptyDir: {}
        - name: db-pass
          secret:
            secretName: mp-demo
            defaultMode: 420
        - name: mp-poi
          configMap:
            name: mp-demo-poi
            defaultMode: 420
      initContainers:
        - name: mp-config-init
          image: 'evolveum/midpoint:4.4-alpine'
          command: ["/bin/bash","/opt/midpoint/bin/midpoint.sh","init-native"]
          env:
            - name: MP_INIT_CFG
              value: /opt/mp-home
          volumeMounts:
            - name: mp-home
              mountPath: /opt/mp-home
          imagePullPolicy: IfNotPresent
      containers:
        - name: mp-pg-demo
          image: 'evolveum/midpoint:4.4-alpine'
          ports:
            - name: gui
              containerPort: 8080
              protocol: TCP
          env:
            - name: MP_ENTRY_POINT
              value: /opt/midpoint-dirs-docker-entrypoint
            - name: MP_SET_midpoint_repository_database
              value: postgresql
            - name: MP_SET_midpoint_repository_jdbcUsername
              value: midpoint
            - name: MP_SET_midpoint_repository_jdbcPassword_FILE
              value: /opt/midpoint/config-secrets/password
            - name: MP_SET_midpoint_repository_jdbcUrl
              value: jdbc:postgresql://mp-demo-db.mp-demo.svc.cluster.local:5432/midpoint
            - name: MP_UNSET_midpoint_repository_hibernateHbm2ddl
              value: "1"
            - name: MP_NO_ENV_COMPAT
              value: "1"
          volumeMounts:
            - name: mp-home
              mountPath: /opt/midpoint/var
            - name: db-pass
              mountPath: /opt/midpoint/config-secrets
            - name: mp-poi
              mountPath: /opt/midpoint-dirs-docker-entrypoint/post-initial-objects
          imagePullPolicy: IfNotPresent
  serviceName: mp-pg-demo

[CAUTION]
In the example the midpoint deployment is not using persistent volumes.
Once only midpoint is restarted (DB is kept running) the new midpoint pod is regenerating keystore.
The result is that the midpoint lost the keys to decrypt encrypted values in DB and it is not possible even to login as administrator.
In case it is just testing environment you have to recreate both DB and midpoint or utilize persistent volume.
Alternative approach is handled in "requirements" to run clustered midpoint - keystore.

=== Cluster - 1+ node(s)

There are few things which has to be handled to be able to operate midpoint in cluster - more cooperating nodes.

* taskManager
* nodeID
* keystore

Once all the "cluster requirements" is met you can increase amount of replicas in statefulset definition for midpoint.

[NOTE]
In case of statefulset the suffix of the pod is increasing order.
First created pod has suffix *-0*.
In case you increase the amount of replicas the pod are added "to the end" of the list *-1*, *-2*, *-3*, etc.
In case you are decreasing the amount of replicas the latest one is removed.
It is not possible to remove the pod "in the middle" of the list.
This may be important in case of utilizing persistent volumes for the pods.

Midpoint pods can be operated even without persistent volumes as the important objects are stored in the repository and shared between the nodes.
The areas which may need specific handling:

* logs +
to not lost the records after removing / re-creating the pod

* connectors +
It can be distributed using shared object (configMap, R/O shared volume between the pods, etc.)

* exports / reports +
In some situation the output can be stored in the filesystem.
In that case we probably prefer to keep the files even after re-creating the pod.

The list is example and it is not have to be complete.
The desing of the deployment may contain other specific objects to handle.

==== Task Manager

Midpoint's task manager has to be run clustered.
This setting has to be added to all nodes.

[source,kubernetes]
...
          env:
            - name: MP_SET_midpoint_taskManager_clustered
              value: true
...

==== nodeID

Node as cluster member has to have the uniq ID.
To be able to run the node there have to be set the way how to generate / set the ID.
As the pods name are uniq (generated) we can use hostname for this purpose.
To do that we need to set one additional evironment variable.

[source,kubernetes]
...
          env:
            - name: MP_SET_midpoint_nodeIdSource
              value: hostname
...

==== keystore

Keystore is generated with the start in case it is not available.
The result is that each node would generate "own" key in the keystore and the object will be readable only by the node which has created it.
To address this "issue" we have to prepare the keystore to be available to each node once it is starting.

* share the file

** network share +
Kubernetes natively offer mount NFS store as volume to pod.
We can share the space and first node will generate it.
All other node or even this node after restat / recreate will use the file so the key for decription will be available.
Using persistent volume for NFS server is good idea.
+
The issue may happen once two pods would starts in parallel and both would want to generate it.

** volume share +
Not all the drivers offer concurent write access to the volume.
This option not necessary have to be available in general.

* pre-generate the file into secret object and share it with all the pods as mounted volume +
For the testing purpose this approach offer sharing the keystore even between the whole env deployment.

===== NFS mount

To mount volume you can use syntax like this example:

.example for NFS volume
[source]
...
    spec:
      volumes:
        - name: nfs-volume
          nfs:
            server: test-nfs.mp-demo.svc.cluster.local
            path: /
...

[NOTE]
FQDN resolution is done by kubelet at the moment of starting the pod.
In case the the node is not able to resolve the FQDN (e.g. it is using "external" DNS server) the mounting may fail.
If you face this issue please change the DNS server in the node's */etc/resolv.conf* or use cluster IP of the service object instead of FQDN.

Once the volume for pod is ready it can be used in container definition in volumeMounts section.

===== keystore generating

In case you prefer to manualy generate keystore the keytool could be used.
Midpoint is expecting jceks format of the keystore.

.generate new keystore (AES_128)
[source,bash]
keytool -genseckey -alias default -keystore keystore.jceks -storetype jceks -keyalg AES -keysize 128

After running of this file you are asked to provide password for the keystore.
This password is the password which is provided to midpoint by using keyStorePassword or keyStorePassword_FILE parameter.
The default is *changeit*.

You can use _storepass_ to set the password for storage as parameter.

.example of generating with the storepass option
[source,bash]
keytool -genseckey -alias default -keystore keystore.jceks -storetype jceks -keyalg AES -keysize 128 -storepass changeit

== Type of the used objects

=== Namespace

To have objects logically groupped the namespace can be used.
The namespace has an impact on generated FQDNs or even permissions in the cloud environment in case they are utilized.

.example of the *mp-demo* namespace definition
[source,kubernetes]
apiVersion: v1
kind: Namespace
metadata:
  name: mp-demo
spec:
  finalizers:
    - kubernetes 

=== Pods

Pods are running instances of the images.
We will not manage them directly as they will be a result of the settings in other objects (like statefulSets).

=== ConfigMaps

The common configuration can be stored in ConfigMaps which can be used in the environment to the definition of the environment variables in the pods or it can be mounted to filesystem.
From the point of view of the pods they are read only objects, what is good in many scenarios.
In case we would need R/W objects, the configmaps are not the right objects to be used (in that case volume may be the option).

.example of the post-initial-objects - new user
[source,kubernetes]
apiVersion: v1
kind: ConfigMap
metadata:
  name: mp-demo-poi
  namespace: mp-demo
data:
  test-user.xml: >
    <?xml version="1.0" encoding="UTF-8"?>
    <user>
      <name>test</name>
    </user>
  test2-user.xml: >
    <?xml version="1.0" encoding="UTF-8"?>
    <user>
      <name>test2</name>
    </user>

Ther configMap can be create in several ways.

* *kubectl apply -f file.yaml* +
This way the content goes also to annotation field in the creating object.
This may sometime cause the issue as the size of the value for the field is limited.
For the common small objects is doesn't cause the issue.
The namespace can be defined as parameter *-n mp-demo* or in the content of the yaml file.

* *kubectl create configmap -n mp-demo --from-file=.* +
This way the annotation field is not created.
If you provide directory as a value of _from-file_ the all the files in the directroty became the keys of configMap and the contents of the files become the value of the keys.
In case you provide just a single file only one key is created in the configMap. +
The content of the kubernetes object is generated so the namespace have to be defined as parameter of the kubectl command.

=== Secrets

For some purposes we need to store the sensitive information like passwords for the environment.
In this case ConfigMaps is not the best object to choose.
The secret object type is used for this purpose.
The content is base64 encoded value.
The short values can be directly shown (decoded) in the tools like link:https://k8slens.dev/[Lens].
In principle, I can provide examples just with placeholder instead of the real values.

.example of the secret object containing x509 certificate with custom CA cert
[source,kubernetes]
kind: Secret
apiVersion: v1
metadata:
  name: cert-mp-pg-demo
  namespace: mp-demo
stringData:
  Expiration: Feb 20 09:04:53 2032 GMT
  SAN: DNS:mp-pg-demo.mp-demo.local, DNS:www.mp-pg-demo.mp-demo.local, DNS:localhost, DNS:localhost.localdomain, IP Address:127.0.0.1
data:
  tls.key: |
    LS0tLS1CRUdJTiBQUklWQVRFIEtFWS0tLS0tCk1JSUV2Z0lCQURBTkJna3Foa2lHOXcwQkFRRUZB
    QVNDQktnd2dnU2tBZ0VBQW9JQkFRQzdpVGs3T2hsMEc0R1YKenBsV25lN0NHSkNMYXg5RkU5VHgz
    Tk1CTnZyTTM5YnNXZmVZR2VDTWRGOG9vMEZ5WXFDZVVnRHZtck9LSXA3RAp2bjltS3puU1JQbVhG
    TnhMZDlYS1VGK1EwVTdWWkE2cURRVWtFeWdZOTJzcEhIRER4ZHNBSGw5T0ZJMGQ3bG1WCmpSR0VW
    ZnZGWklBUE1EdGZnWE1heTZpN3FYWEdHdE13Mjd4dXQ5U2Qvb2xZYlRYMloxNjl5V0ZuTXJKd0Ji
    dlIKUjJFQTVpUHRyY3M4ekVaNi91MHRBSURia3NJdlFaMk45bWRuT1NUS0ZoQnVxUlNhM3JBQXFa
    TjZ3WGNEUHk1YgpzVXBZaG4yUXVTdGRVL3ZHK0lZanVCcjVkaXRZejVEWG5YbDV4UmN2ZGlZNE9m
    OVowVnZBWktjQmVYMlpBakptClkxRUN3ZTVMQWdNQkFBRUNnZ0VBQ0VkRmRIemJGTW9HRFdQaVZ2
    Vi96YWVoOGRXWHVzZm1WWXJtOFozSXNuU0wKRzBIWmNTaHJSaWY0NzJWTmhVd2ZSUDNmdHJRQWl1
    OStUS3ZER2ZKOGlmQ3JoK1RPMkxJQWlQN0ZTVEpFSloxVwpIR0dZd1gvcS9EY256dGZIam96VVh3
    djRMdzB2TVl3TS9sSU0zT2VpeHBMcUtFRjd6WHA4WjBsb09HaUJuMjRLClZsL0wwb3dDVzc4bFNy
    ME9BcU5KN1krWDQ5aGQ4T05KdlYwSUtZQm5MbEVIUGQwSysxL3ZsdmJQNkMwSS9TNkkKYXZ0MXlu
    bHQ2MVV1eEVCZzVUQTZHWEJnQ2Y2N01sWWMyRDJoSHJjaDNMdlY1bjVocVdTaXNJblpiR0xNSTdD
    cgoycW1uSDB1QlpqR0FLaG5xNHdWUDZSVmJDRSsrejYxTHMrMDd5dS9Eb1FLQmdRRG41NDlYaDdE
    dlV4WnZIS3hECjlURi9yNmlHeXIyY3VsN0tpOThYTUFZdXFsQmwxVDNNMTB4UnQzNGdYbTBOSjZs
    a0xjRUhxTEhsdWZPQ3U4MlMKMzZ3NmYwd1NMYUo3ckJlZlNOK3EyNmFObVlkcW9iclZQUWc1R0ti
    Q1E3UWYvOTViMTc5S3V5UFpEdDQvQTJUZwowQ294S1FVWUVnd1hXRWowTUJmQ3R3RlEyd0tCZ1FE
    UEJZRHdnaW55SlNwZ3JtZkZLdk4wUXYrNEQ3eUR4SllwCmxNMTVTWnpwUU41ZFJlZkdVcVlMZnRx
    R0dSSWk2aFRhU3VTeEVtV1Y1R21EWlQxR1RmMkVVQWpkNnIyM0JVWS8KcXMxQlRmUDVMaXhNaEZz
    YkRoMlpxT0tnb1pBVy9DSXVtVU9GcFZyVkIwdFQvc0lZV1ZIMUJTcEhCZFczdHRqNgpKTjVMMDBU
    YlVRS0JnUURRNWFNV3l0RlVWRGtpTCtieGRINTVNYWVTZkRDZmJMYVVwN2gzTDdoeDh0Tm5WOVpL
    Cm5pcE9kZ2IwYTNxNlhkN1ZzZTF1MDhRa1Z3UUc0TnVHa1Z1WWVqRHhNeEJwYlJUK1UrdS9BZ3Vm
    QkF5YXZrTUoKY09mbVh0RmdyeElqdlIzMUIzeDJlZXNKekwzZ2IxTkF1K00xMDJpV1RUTjlGL2Ju
    bUNiZkxIRFAvd0tCZ1FDaQp6REF1Y2gyc1J4L0JNY1YyM3FUZXE0Vk82RWtWMGZWSU03VTFpUGIw
    MHNkSzBCdEk4VnVVTktpQnhadG1pMi9rCmpmcXphTVVzRDVnTjlRSXZsRXNsem81NmZRdTlyazhr
    NkJ0TEhKTWNRL3dnSEFGTkVGZWtxNU8wUC9rQXFqdnoKS3lGWEtzWjRPYWs2SDEvaEovUjBXeUI1
    QTVTaVNRN3QveW8wdEtvMFVRS0JnRXlKMDAzYVZjK2MxSHhHaTNEWApKZDUyL1d5YXVvemZhQVFx
    djd4UE9EdXNOSGdmRStIS3pyNHZpckcxbkVNaWE3ZTZldEtLeW9YRldmT3RBTmZNCnBnODZYUEhO
    QmJnR3lYTjY1YVhvdHdqbHVXSXhCVkU5QkVRRGRKZjBRd3JvaGpnTkZIUm1sUzlYL05UZWQ4YWUK
    QmdlazlZdlBOOHd5WTFkSzNSOVB2TTZPCi0tLS0tRU5EIFBSSVZBVEUgS0VZLS0tLS0K
  tls.crt: |
    LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUR4RENDQXF5Z0F3SUJBZ0lVZkltdTZIUTE0
    NXNPbjFyYTZwWW45S2dJUXprd0RRWUpLb1pJaHZjTkFRRUwKQlFBd0ZERVNNQkFHQTFVRUF3d0pa
    R1Z0YnlCalpYSjBNQjRYRFRJeU1ESXlNakE1TURRMU0xb1hEVE15TURJeQpNREE1TURRMU0xb3dG
    REVTTUJBR0ExVUVBd3dKWkdWdGJ5QmpaWEowTUlJQklqQU5CZ2txaGtpRzl3MEJBUUVGCkFBT0NB
    UThBTUlJQkNnS0NBUUVBdTRrNU96b1pkQnVCbGM2WlZwM3V3aGlRaTJzZlJSUFU4ZHpUQVRiNnpO
    L1cKN0ZuM21CbmdqSFJmS0tOQmNtS2dubElBNzVxemlpS2V3NzUvWmlzNTBrVDVseFRjUzNmVnls
    QmZrTkZPMVdRTwpxZzBGSkJNb0dQZHJLUnh3dzhYYkFCNWZUaFNOSGU1WmxZMFJoRlg3eFdTQUR6
    QTdYNEZ6R3N1b3U2bDF4aHJUCk1OdThicmZVbmY2SldHMDE5bWRldmNsaFp6S3ljQVc3MFVkaEFP
    WWo3YTNMUE14R2V2N3RMUUNBMjVMQ0wwR2QKamZablp6a2t5aFlRYnFrVW10NndBS21UZXNGM0F6
    OHVXN0ZLV0laOWtMa3JYVlA3eHZpR0k3Z2ErWFlyV00rUQoxNTE1ZWNVWEwzWW1PRG4vV2RGYndH
    U25BWGw5bVFJeVptTlJBc0h1U3dJREFRQUJvNElCRERDQ0FRZ3dDUVlEClZSMFRCQUl3QURBZEJn
    TlZIUTRFRmdRVVR0dVJIdGE4S0RuanVHenBWOEYwYy9uQXBtd3dUd1lEVlIwakJFZ3cKUm9BVVR0
    dVJIdGE4S0RuanVHenBWOEYwYy9uQXBteWhHS1FXTUJReEVqQVFCZ05WQkFNTUNXUmxiVzhnWTJW
    eQpkSUlVZkltdTZIUTE0NXNPbjFyYTZwWW45S2dJUXprd0N3WURWUjBQQkFRREFnWGdNR2tHQTFV
    ZEVRUmlNR0NDCkdHMXdMWEJuTFdSbGJXOHViWEF0WkdWdGJ5NXNiMk5oYklJY2QzZDNMbTF3TFhC
    bkxXUmxiVzh1YlhBdFpHVnQKYnk1c2IyTmhiSUlKYkc5allXeG9iM04wZ2hWc2IyTmhiR2h2YzNR
    dWJHOWpZV3hrYjIxaGFXNkhCSDhBQUFFdwpFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUhBd0V3RFFZ
    SktvWklodmNOQVFFTEJRQURnZ0VCQUNYaHFRbEhkbk5GClhQNXJIR0JNOThQU0g3TVplYXZjUW5C
    RjNUNExEZ0NYZUkrK2FiNnNVcEZhSkFHZHgxNTZnZDZzTHI5T3lWNWgKTTJDbG9OVDBvbVhrSk9J
    MXVEQUp5cVV1OVJENDdOaUxEZjZ5cjl6OTF0NE8xcEF1NHJjV2FhS1Qvd3FWY0dkYQpXUTBtQmdO
    Z0pMVytUd3NUN3JuOTNtZGtoRUlWWHFtamhOTmQ2bmZEZGdrcG96WkNTUFFrZHQ4SWxsS2RQdnJD
    CkFxNHdCbERIektTSUlobkRGYTArMURVbkFBVFk5ZFJFQVJHUTVTTWpGZjJqNStrL21ySCt2cGtT
    eDdRRDByUXQKUms1YkRlNmdWZjZFMXhsNnM4ZTZVbjY1eG5jRjdrZW9EVURIVkhzNmQxNm5oZExB
    Mi95T01LU2RpYzJFYmZZQgo4V2pYSGJ0N085TT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
type: kubernetes.io/tls

[source]
----
...
stringData:
  Expiration: Feb 20 09:04:53 2032 GMT
  SAN: DNS:mp-pg-demo.mp-demo.local, DNS:www.mp-pg-demo.mp-demo.local, DNS:localhost, DNS:localhost.localdomain, IP Address:127.0.0.1
...

These lines are optional just for better overview - information about the containing certificate.
----

.commands to generate self sign certificate
[source,bash]
----
# generate the key and the certificate
openssl req -new -sha256 -newkey rsa:2048 -keyout tls.key -nodes -subj "/CN=demo cert" | openssl x509 -req \
-signkey tls.key -out tls.crt -days 3650 -sha256 -extfile <(cat <<EOF
basicConstraints = CA:FALSE
subjectKeyIdentifier = hash
authorityKeyIdentifier = keyid,issuer:always
keyUsage = digitalSignature, nonRepudiation, keyEncipherment
subjectAltName = DNS:mp-pg-demo.mp-demo.local, DNS:www.mp-pg-demo.mp-demo.local, DNS:localhost, DNS:localhost.localdomain, IP:127.0.0.1
extendedKeyUsage = serverAuth
EOF
)

#create secret object for kubernetes
kubectl create secret tls -n mp-demo cert-mp-pg-demo --cert=tls.crt --key=tls.key

# show the content of the certificate
openssl x509 -in tls.crt -text -noout
----

.example of the secret object containing the passwords
[source,kubernetes]
apiVersion: v1
kind: Secret
metadata:
  name: mp-demo
  namespace: mp-demo
data:
  password: U3VwZXJTZWNyZXRQYXNzd29yZDAwNw==
type: Opaque

[NOTE]
In the secret object the values are provided as base64 encoded content.
For our example we have following values: +
SuperSecretPassword007 => U3VwZXJTZWNyZXRQYXNzd29yZDAwNw==

=== Services

As the pods are in principle dynamic objects, the IPs are changing each time the pods are recreated.
To have "stable" point for interaction between the pods, the services are defined.
The service looks for the pods based on the label.
The service itself has assigned IP.
The traffic is "forwarded" to the pods relevant to the service based on the label selector.

One example has been already provided related to the deployment with postgresql DB.
Other example may be for the midpoint itself.
Here is example ready for the cluster environment.
The difference is in Session Affinity setting.

.example for the midpoint service (cluster ready)
[source,kubernetes]
apiVersion: v1
kind: Service
metadata:
  name: mp-pg-demo
  namespace: mp-demo
spec:
  ports:
    - name: gui
      protocol: TCP
      port: 8080
      targetPort: 8080
  selector:
    app: mp-pg-demo
  type: ClusterIP
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800

[NOTE]
SessionAffinity is necessary for the midpoint as the session is not shared between the nodes of the cluster.
In case there is just one node the missing of the affinity setting is not critical.
Once there are more than 1 node the missing of the affinity setting cause loop of the login process. +
The reason is that after sending login information the session is created with one node but the next communication is handled by other node - the default is round-robin distribution of the communication.
This other node doesn't know anything about just created session on previous node so the redirect to login page occur.

=== Ingress

To be able to reach the services from outside on shared ports (80,443) there is ingress in place.
It utilizes SNI, which is nowaday automatically used so there is not additional requirement.
We are defining the rules for the conditional traffic forwards to the specific service and port.

.example of the ingress object definition (assumption: *mp-demo.local* domain)
[source,kubernetes]
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: mp-pg-demo
  namespace: mp-demo
spec:
  tls:
    - hosts:
        - mp-pg-demo.mp-demo.local
      secretName: cert-mp-pg-demo
  rules:
    - host: mp-pg-demo.mp-demo.local
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: mp-pg-demo
                port:
                  number: 8080

=== StatefulSets

This is the glue for all.
This object defines the setting for the future pods and handle the amount of replicas.
In case some pod will fail, the StatefulSet definition will handle the situation and recreate the new one.

== Tips

=== custom path in URL

Default path in URL is /midpoint.
It is possible to change the path using *application.properties* file in midpoint.home location.

.midpoint.home/application.properties
[source]
server.servlet.context-path=/xyz

This will change the URL to /xyz instead of /midpoint.

To set up we can use init container which is already used in case od native repository.
Instead of directly run the *midpoint.sh* file we can run "script" containing set ip the necessary value next to midpoint.sh execution.

.Original code for init container for midpoint pod
[source,kubernetes]
...
          image: 'evolveum/midpoint:4.4-alpine'
          command: ["/bin/bash","/opt/midpoint/bin/midpoint.sh","init-native"]
          env:
...

.Customized code covering also change of the path in URL
[source,kubernetes]
...
          image: 'evolveum/midpoint:4.4-alpine'
          command: ["/bin/bash","-c"]
          args: ["/opt/midpoint/bin/midpoint.sh init-native; echo 'server.servlet.context-path=/xyz' >/opt/mp-home/application.properties"]
          env:
...

== References 

* xref:/midpoint/install/docker/native-demo.adoc[Native repository demo]
* xref:/midpoint/install/midpoint-sh.adoc[start script]


