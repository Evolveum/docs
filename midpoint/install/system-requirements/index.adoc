= MidPoint System Requirements
:page-nav-title: System Requirements
:page-wiki-name: System Requirements
:page-wiki-id: 3145846
:page-wiki-metadata-create-user: mamut
:page-wiki-metadata-create-date: 2011-09-27T13:44:16.115+02:00
:page-wiki-metadata-modify-user: petr.gasparik
:page-wiki-metadata-modify-date: 2020-07-15T11:06:14.784+02:00
:page-upkeep-status: green
:page-toc: top
:page-description: Recommended system sizing and infrastructure configuration for midPoint deployment in various scenarios
:page-keywords: system requirements, sizing, infrastructure, clustering

This document describes system requirements for midPoint and its components.

== Deployment Architecture

A midPoint deployment consists of the following units:

* <<midpoint_server_sizing,MidPoint server>>

* <<database_system_sizing,Database system>>

* <<connector_servers_sizing,Connector servers>> (optional)

Each deployment unit has particular characteristics and requirements as described in the sections below.

[NOTE]
====
It is impossible to provide precise midPoint platform sizing and parametrization
because it vastly depends on your particular usage patterns,
number and nature of running tasks and connected resources,
as well as synchronization and workflow processes.

This article gives general rules as well as estimates based on our experiences from typical deployments.
You are expected to adjust the parameters based on your specific situation and IT operation best practices.
====

[[midpoint_server_sizing]]
== MidPoint Server Sizing

The midPoint server is the core component of any midPoint installation.
It handles the majority of user interactions and data processing, making it particularly sensitive to CPU load and RAM size.
The table below summarizes the recommended resources for typical scenarios:

// TODO: Are the disk size values still valid for 4.9+, with all the new caching?
//(or probably the DB disk size, but the question stands) 2025-07-08 @dakle
.Recommended MidPoint server parameters
[%autowidth]
|===
|  | Minimum | Less than 5,000 users | 5,000 - 50,000 users | 50,000 – 100,000 users | More than 100,000 users

| CPU
| 1 core
| 4 cores
| 8 cores
| 16 cores
| Custom sizing ^*^


| RAM
| 4GB
| 8GB
| 16GB
| 16GB
| Custom sizing ^*^


| Disk space
| 2GB
| 10GB
| 20GB
| 40GB
| Custom sizing ^*^


| Disk I/O
| negligible
| negligible
| negligible
| negligible
| Custom sizing ^*^


|===

^*^ Deployments with more than 100,000 users are supported in midPoint.
However, due to the variability in large-scale environments, it is recommended to consult the specific hardware requirements with link:https://evolveum.com/services/consulting-services/[Evolveum Support].

=== Number of MidPoint Server Instances

The sizing table above assumes a single midPoint instance (node).
Multi-node midPoint instances are usually deployed for high availability reasons, and they are usually two-node systems.
Each node needs to be sized to handle the full load of the system, therefore each node should be deployed according to the midPoint server sizing requirements outlined in the table above.

Another reason for multi-node deployments is to isolate synchronous load (e.g., user interactions via GUI or REST API) and asynchronous load (e.g., tasks and processes).
However, there is no universal rule for sizing such a system, meaning you need to do a case-to-case analysis and measurements.
The values provided in the table above may serve as a starting point.

See also:

* <<high-availability,Details about high availability options>>
* <<multi-node-deployment,Multi-node deployment system requirements>>

[[database_system_sizing]]
== Database System Sizing

// TODO reference native repo docs, when sizing is written for it:
// xref:/midpoint/reference/repository/native-postgresql/postgresql-configuration/#db-server-sizing[]
// checked on 2025-07-08, not written yet. @dakle

The repository database stores the vast majority of midPoint data.

The load on the database system depends mostly on the size and nature of the data, usage patterns, and the type and configuration of the database system used.
Use the values below as general guidelines and adjust them to fit your specific use case.
For a more precise estimate, contact link:https://evolveum.com/services/consulting-services/[Evolveum Support].

.Recommended repository database server parameters
[%autowidth]
|===
|  | Minimal | Less than 5,000 users | 50,000 – 100,000 users | More than 100,000 users

| CPU
| 1 core
| 2 cores
| 8 cores
| Custom sizing ^*^

| RAM
| 2GB
| 3GB
| 12GB
| Custom sizing ^*^

| Disk space
| 1GB
| 5GB
| 20GB-100GB for 3 months audit
| Custom sizing ^*^

| Disk I/O
| small
| medium
| medium to high
| Custom sizing ^*^

|===

^*^ Deployments with more than 100,000 users are supported in midPoint.
However, due to the variability in large-scale environments, it is recommended to consult the specific hardware requirements with link:https://evolveum.com/services/consulting-services/[Evolveum Support].

The recommended values assume that beside storing operational data, you set up midPoint to keep only a reasonably small amount of historical data, such as audit records.
If you plan to store historical data in midPoint for longer time span, you must account for that when sizing the database.

[[shared-vs-dedicated-database]]
[TIP]
.Shared vs dedicated database
====
The xref:/midpoint/reference/repository/native-postgresql/[native repository] of midPoint relies on PostgreSQL.
We recommend using a separate database server even for smaller midPoint deployments.
This is default for xref:/midpoint/install/#containerized[containerized installation].
You may also consider two separate databases for xref:/midpoint/reference/security/audit/#separate-repository-configuration-for-audit[audits] and for the repository, as the access patterns and sizing requirements are quite different.
====

[[connector_servers_sizing]]
== Connector Servers (Optional)

xref:/connectors/connid/1.x/connector-server/[Connector Servers] are small software components that act as a proxy for connectors that xref:/connectors/connid/1.x/connector-server/#why-use-a-connector-server[cannot run inside midPoint].
Deployments of these components are quite rare.

Resource requirements of connector servers are usually negligible:
a tiny portion of CPU and RAM, and a disk space measured in megabytes.
We strongly recommend deploying these components on shared servers.

== Your Options for High Availability Deployment

There are several approaches to implement high availability (HA) for midPoint deployments.
Each strategy has different characteristics and costs:

* <<virtualization_based_HA,Virtualization-based high availability>>

* <<load_balanced_shared,Multiple-node deployment>>

[TIP]
.Is midPoint business-critical for you?
====
MidPoint is an identity management system, and as such, it is seldom a business-critical system.
If midPoint fails, the impact is usually negligible.
The integrated systems (resources) are independent of the midPoint instance by design.
Therefore midPoint failure does not influence the operation of such systems in any significant way.
A midPoint failure can influence identity management capabilities, password resets etc.
But these functions are usually *not critical* for operation, especially if the outages of midPoint are short (minutes).
Even longer outages (hours) do not usually impact operation of the infrastructure in any significant way.
This is important to keep in mind when choosing the right HA strategy.
====

[[virtualization_based_HA]]
=== Virtualization-Based High Availability

The easiest way to implement high availability (HA) is to use the HA features of the underlying virtualization infrastructure.
If the host machine running the midPoint virtual machine fails, it is easy to move the entire virtual machine to a different host.
The transfer inflicts some downtime (usually a few minutes).
However, since midPoint is not a business critical system, this downtime is generally acceptable.

In this scenario, midPoint is set up to run in a single-node configuration (default), and no extra configuration is necessary.
You only need to set up backups at adequate frequency to minimize data loss in case of fire.
The high availability and failover mechanism is completely transparent.
MidPoint has internal mechanisms to recover from system outages which are utilized after midPoint restarts on a different machine.

This is a cost-efficient failover strategy, especially if midPoint and the database run on the same virtual machine.

[[load_balanced_shared]]
=== High Availability Through Multiple Nodes

This approach relies on deploying multiple instances (nodes) of midPoint and using a standard HTTP load balancer at the HTTP layer (in the link:https://en.wikipedia.org/wiki/Load_balancing_(computing)#Persistence[sticky mode]).
All midPoint nodes connect to the same database.
Depending on xref:/midpoint/install/#installation-options[how you installed midPoint], you may choose to share the database with other systems or dedicate it to midPoint.
As <<shared-vs-dedicated-database,mentioned above>>, we always recommend using a dedicated database for midPoint.

Refer to the <<multi-node-deployment,section on multiple-node deployment>> for details.

=== Database High Availability

MidPoint does not explicitly support repository-level clustering.
Even in big deployments, the bottle neck is usually the amount of resources or the effective communication speed of the resources.

We suggest to primarily *set up a proper backup solution*, verify the time to recovery, and always xref:/midpoint/reference/simulation/[*simulate everything* before you make changes to production].

If you require clustering the DB, you can set up an link:https://www.geeksforgeeks.org/system-design/active-passive-active-active-architecture-for-high-availability-system/[active-passive] failover mechanism and take advantage of the PostgreSQL database-side clustering support on the JDBC driver level.

// This is best covered in [midPoint] Advice on Database High Availability --  https://lists.evolveum.com/pipermail/midpoint/2024-May/008167.html

== Software and Infrastructure Requirements

When starting an IAM project, you must prepare not only the midPoint servers but also the database and load balancer (if required).
You need access to the infrastructure where these servers are running, as well as to the source and target systems.

Refer to the xref:/midpoint/release/[midPoint Releases] documentation for software requirements.

=== Basic Single-Node Deployment

The following schema represents a basic deployment environment:

.Basic single-node midPoint deployment
image::midpoint-environment-schema-basic.svg["Basic midPoint deployment schema with midPoint and its repository in the center; lines showing access routes to example source and target resources and user's computer. Optional VPN is placed between midPoint and the computer"]

In the center of the schema, the largest rectangle represents a virtual machine, usually running Linux, with the xref:/midpoint/install/bare-installation/distribution/#purpose-and-quality[basic required set of tools] installed.
MidPoint runs on this virtual machine.
The database repository preferably runs on a separate virtual machine, albeit possibly on the same physical server.

==== Shared Database Repository

For the database repository (DB), you can use an existing DB server or set up a new dedicated one.
The midPoint server needs to have access to it using SQL via TCP/IP.
Do not forget to configure the firewalls to enable communication over the used ports.

The database needs to be configured prior to midPoint installation so that the person who installs midPoint can configure the midPoint instance correctly.
If you deploy in containers, see xref:/midpoint/install/containers/#deployment-schema[] for details.

==== Notification System

If e-mail notifications are needed, access to the SMTP server and an account with send privileges is required.
If you need SMS notifications, you also need access to an SMS gateway and have the account privileges to send SMS.

==== Secure Access to Your Deployment

If your setup requires access to your midPoint deployment over the public Internet, you need to secure the communication between end users' computers and the midPoint instance.
One of the options is to use a virtual private network (VPN) for everyone who needs to access midPoint.
VPN provides a tunnel from the user's machine to midPoint (or the load balancer before it).

==== Troubleshooting Access to Nodes

You can set up an SSH access to your midPoint nodes.
You can use it to access midPoint configuration files and logs in case something goes wrong.
If you deploy on xref:/midpoint/install/containers/kubernetes/[Kubernetes] and use xref:/midpoint/operations-manual/#_syslog_logging[remote logging], you do not need SSH access to individual nodes to manage, diagnose, and fix them remotely.

==== Resources

There are many different communication protocols the resources may use.
The schema above shows just a couple of the most common ones.

You may have an HR system which is only able to give midPoint CSV files, there may be more complex systems, data of which you can access using SQL, you may need to connect an Active Directory, and so on.
With some resources, such as xref:/connectors/connectors/com.evolveum.polygon.connector.sap.SapConnector/[SAP and JCo], you need to enable API on the target resources, open your firewall on the servers where the target systems run, and create an account with appropriate permissions to manage identities.
In certain cases, you may have to deploy a <<connector_servers_sizing,connector server>> to access some special resources.
The list of options is endless and very much depends on what exactly you need to manage with midPoint.

=== Multi-Node Deployment

The situation gets a bit more complex when you deploy midPoint on two or more nodes:

.Deployment of midPoint with two nodes, a load balancer, optional VPN, and a few example resources
image::midpoint-environment-schema-HA-two-nodes.svg["MidPoint deployment schema with two midPoint nodes and their shared repository in the center; lines showing access routes to example source and target resources and user's computer. Load balancer and an optional VPN are placed between midPoint and the computer"]

==== Work Distribution

In multi-node deployment, the job distribution among the nodes is handled centrally.

The repository DB keeps track of worker task states, i.e., which are to be done, in progress, or done.
MidPoint uses the Quartz job scheduling library on each node.
The Quartz library instances use the xref:/midpoint/reference/tasks/task-manager/configuration/#jdbc-scheduler-job-store[central JDBC scheduler job store] to ensure that no single task is processed by multiple nodes concurrently.
The available worker tasks are picked by the nodes on the first-come-first-served basis, which ensures a reasonably even task distribution.

Refer to xref:/midpoint/reference/tasks/task-manager/[] for details on task handling in midPoint.

==== Communication Among Nodes

The nodes communicate primarily with the central repository database rather than among themselves.
One exception is *cache invalidation*.
When a node changes data in the midPoint database, the node informs other nodes about the need to invalidate their cache.
The *communication between nodes runs over HTTPS*.

Refer to the xref:/midpoint/reference/deployment/clustering-ha/#intra-cluster-communication[article on high-availability deployment] for details about node communication.

==== All Nodes Are Created Equal

All nodes need to have the same configuration and access levels regardless of whether you deploy midPoint on 200 nodes or just one.
There must be no differences because *all nodes are created equal*: when one node goes down, others need to replace it in full.

You can check that connections to resources work as expected using `ping`, `telnet`, or `wget`, for instance.

== Environment Requirements

Usually, at least two environments are typically used for the development of an IAM deployment: test and production.
In many cases, there is also a local midPoint installation on the identity engineer's computer and a separate development environment in the customer's infrastructure.

=== Keep the Environments as Similar as Possible

The best practice is to use a configuration that is as similar as possible in all these environments.
However, the environments should also be completely isolated so that the test environment cannot touch production data on a resource.
VPN can be shared.

We recommend having the same operating system, midPoint version, and resource data for all environments.
If _same_ is not possible, as similar as possible is desirable.
Any differences may lead to situations where something works and is well tested in one environment, but does not work in another.

Regardless of how similar you can keep your environments,
*we suggest you xref:/midpoint/reference/simulation/[simulate every change]* on the production environment before you deploy it.
Using simulations, you can discover issues before they can do any damage to your production data.

If the data you manage with midPoint are sensitive and cannot be used in the development environment, you can obfuscate them and only use a part of them as a sample.
However, the schema and all attributes that you use need to be used the same way as in the production environment to minimize differences.

When deploying the solution to production, you need to have access to the production environment and the data there.
In this case, it is not necessary to obfuscate data for the test or development environments, because the same identity engineer is responsible for the development, testing and deployment.

[WARNING]
====
Irrespective of whether you use the original or obfuscated data, you need to be able to use production data in the development phase to prevent future issues.

Note that running an IDM project involves consolidating users, changing the data structures, and accessing data in general.
Every discrepancy and exception will surface in production, and you will need to decide how to handle it.
That is why it is best to implement your IDM project when you are not doing acceptance testing, have problems in production, or face hard deadlines.
====

== See Also

* xref:/midpoint/release/[midPoint Releases]

* xref:/midpoint/reference/repository/native-postgresql/[]

* xref:/midpoint/reference/deployment/clustering-ha/[]
