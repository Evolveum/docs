= MidScale Design Meeting Overview
:page-toc: top

== Schroedinger

Improvement of Schr√∂dinger GUI testing framework.

* New testing classes/methods
* New tests
* Driving the tests (Jenkins)
* Cleanup code
* Documentation
* Integration with Studio?

Lead: Kate +
Members: Katka, ???, Vilo (advise)

== User Interface (GUI)

Performance, scalability and improvements to midPoint administration user interface (`admin-gui`).

* Diagnostics and improvements of performance issues.
* Need to re-structure the code?
* Scalability features and UX: Small improvement to support administration of massive data.

Lead: Katka +
Members: Kate, ???

== Repository

MidPoint database data store (`repository` subsystem).

* New PostgreSQL repository implementation.
* Data migration, `ninja`
* Human-friendly query language (and Axiom-compatible)

Lead: Riso +
Members: Palo, Tony, Vilo (advise), Rado (advise)

== Tasks

MidPoint task management (especially in clustered environment). Cluster management.

* Multinode tasks, e.g. automatic scaling of tasks in the cluster.
* Cluster autoscaling, i.e. dynamically add/remove cluster nodes

Lead: Palo +
Members: Katka, Kamil (test), ???

== Infrastructure

Improving Evolveum testing lab, setting up automated scalability testing environment.

* Testing environment architecture, setup.
* End-to-end testing - how to do it, how much to do it, what scenarios.
* Driving tests from Jenkins.
* Performance testing environment.
* Profiling tools.
* Bulk data (quick initialization)

Lead: Igor +
Members: Patrik, Kamil, Riso (advise/jenkins), Palo?, Vilo(advise)?, ???

link:../infrastructure-design/[Meeting notes]

== Prism

Improvement to Prism, fundamental data representation layer of midPoint.

* Thread safety.
* Identification of performance issues (bottleneckes): profiling.
* Immutability concepts (for stability)
* Axiom next steps (only the necessary steps)

Lead: Tony +
Members: Palo, Rado (advise)

== Testing

Definition and coverage of the functionality by testing scenarios. 
We prefer to automate whenever applicable, but we need at least informal definitions to make sure our requirements are covered and tested. 

Would be organized on the tail of the design meetings.

* Compile requirements from streams
* Define structure of test cases and tests
* Agree on execution, reporting and evaluation, prepare a landing for all tests across streams
* Upgrades/Migration testing
* Keep in mind large data sets - focus for all test scenarios

Lead: Igor +
Members: Tony, Palo, Riso, Katka, Kate, Kamil

== Other Activities

* Acceptance testing. Lead: Igor.
* Documentation. Lead: Rado.


