= Testing Design
:page-toc: top

This document covers only new tests needed for midScale.
These are mostly performance tests, typically comparing state before and after.

The baseline for the tests can be obtained using:

* The current 4.2 version or the head of its support branch (little to no performance improvements are expected in the branch).
* "Old" implementations of components still available in the `master`, e.g. current Hibernate-based repository.

This means that some performance comparison tests may be executed in the `master` comparing
two flavours of the component (like the repository).
Other tests will be run both on the `support-4.2` branch and the `master` comparing the same parts
before and after the improvements.

For consistency reasons, the baseline will be measured on `support-4.2` even for cases
where the old implementation is preserved in the `master`.

== Tooling

TestNG already used for unit and integration testing will be used also for performance tests.
MidPoint runs tests in multiple modules, often testing anything below the currently tested module.

* For Prism tests we can add more unit-style tests in the Prism module.
These do not use Spring framework and have minimal overhead.
* For pure repository testing we can implement them in `repo-sql-impl-testing`.
These initialize Spring context with the repository and dependent components, but without higher-level
midPoint components like provisioning, model or GUI.
* GUI and end-to-end integration testing will be supported by our Schr√∂dinger module,
effectively running midPoint in its entirety.
* Cluster - TBD

Tooling must work both for testing on some reference setup and when run on the developer's machine.
Obviously, results can only be compared with other results in the same environment.

== Performance test output

* Monitor name conventions?
* Distinguish test and production monitors?
* The metric name is test+monitor name.
* How do we want to identify the run?
Jenkins job run? Test run date? Git commit ID? Commit date? Branch? Symbolic name (M1, etc.)?
* How to chart it?
This can be work for infrastructure team.
* Results must be postprocessed somehow - first there are various monitors from single run.
We want to add each monitor to its time series (results of multiple runs).
Here the run identification is important.
